{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaPV1GllMBDIPqIDmfs6Ie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mudassir-beep/AI-Disease-diagnostics/blob/main/RAG-Faiss-Tiny-Lama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Install necessary packages\n",
        "!pip install -q streamlit pyngrok pandas torch transformers langchain sentence-transformers\n"
      ],
      "metadata": {
        "id": "FmeWleHJzXmX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q \\\n",
        "    streamlit pyngrok pandas torch transformers \\\n",
        "    sentence-transformers langchain langchain-community faiss-cpu\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eez5Mq2w4rYO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok pandas torch transformers sentence-transformers langchain langchain-community\n"
      ],
      "metadata": {
        "id": "QykJGKpT1rMx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLK1u7soy16y",
        "outputId": "59a31ab2-f7c5-4d14-d7ea-8fdac0c7b373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import Document\n",
        "\n",
        "st.set_page_config(page_title=\"TinyLlama RAG Chatbot\", layout=\"wide\")\n",
        "st.title(\"ðŸ§  TinyLlama RAG Chatbot\")\n",
        "\n",
        "# âœ… Path to uploaded CSV file\n",
        "file_path = \"/content/gaza_genocide_docs_clean.csv\"\n",
        "\n",
        "# âœ… Load CSV\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'text' not in df.columns:\n",
        "        st.error(\"âŒ CSV must contain a column named 'text'.\")\n",
        "        st.stop()\n",
        "    st.success(f\"âœ… Loaded {len(df)} rows from: {file_path}\")\n",
        "except Exception as e:\n",
        "    st.error(f\"âŒ Failed to load CSV: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# âœ… Convert to LangChain Documents\n",
        "try:\n",
        "    documents = [Document(page_content=str(row)) for row in df['text'].dropna()]\n",
        "    if not documents:\n",
        "        st.error(\"âŒ No text found in 'text' column.\")\n",
        "        st.stop()\n",
        "except Exception as e:\n",
        "    st.error(f\"âŒ Error converting DataFrame to documents: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# âœ… Load TinyLlama Model on GPU (if available)\n",
        "with st.spinner(\"â³ Loading TinyLlama...\"):\n",
        "    try:\n",
        "        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ Model loading failed: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "# âœ… Build FAISS Vector DB using GPU\n",
        "with st.spinner(\"ðŸ” Building FAISS Vector DB...\"):\n",
        "    try:\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "        db = FAISS.from_documents(documents, embedding=embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "    except Exception as e:\n",
        "        st.error(f\"âŒ FAISS or RAG failed: {e}\")\n",
        "        st.stop()\n",
        "\n",
        "# âœ… UI for asking question\n",
        "user_question = st.text_input(\"ðŸ’¬ Ask a question:\")\n",
        "if user_question:\n",
        "    with st.spinner(\"ðŸ¤– Generating answer...\"):\n",
        "        try:\n",
        "            response = qa_chain.run(user_question)\n",
        "            st.markdown(\"### âœ… Answer:\")\n",
        "            st.write(response)\n",
        "        except Exception as e:\n",
        "            st.error(f\"âŒ Answer generation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 30Hp3ggHO2VyEkOtL7XXIrnzgBv_2dhqUQVo6XVQzzgpt5McZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnUc376zy8Jx",
        "outputId": "843c9988-6030-4118-ebd0-fd7cc8e30759"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n",
        "!pkill -f ngrok\n",
        "# âœ… Launch Streamlit via ngrok in Colab\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "def run_app():\n",
        "    !streamlit run app.py &\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to spin up\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"ðŸ”— Public URL:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3cvCs_KzC9q",
        "outputId": "652fce9d-8cd6-449c-abb2-c8c427b46003"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.168.242.86:8501\u001b[0m\n",
            "\u001b[0m\n",
            "ðŸ”— Public URL: NgrokTunnel: \"https://e39c1d09d4e5.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}